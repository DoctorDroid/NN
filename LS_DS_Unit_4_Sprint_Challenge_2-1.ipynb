{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.23.3"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Q99Oonun0A"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Simple Perceptron](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron\n",
        "    - Analyze and Compare\n",
        "4. [Keras MMP](#Q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28IOvcoun0B"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Defining Neural Networks \n",
        "\n",
        "Write *your own* definitions for the following terms:\n",
        "\n",
        "- **Neuron:** The Nodes of a Neural Network. These represent the values after a calculation is made depending on the weights that procede it and then pass the sum of those calculations to the next layer.\n",
        "- **Input Layer:**The first layer of a nueral network. These represent the values before any weights are caluculated. This layer requires an input dimension.\n",
        "- **Hidden Layer:**The layers between the input layer and the output layer. there is at least one hidden layer and can be as many as designated by the creator of the network. \n",
        "- **Output Layer:**This layer represents the finalized value of the calculations performed after the input layer and by each of the hidden layers. The amount of Neurons here should match the amount of classes in a multiclassification model (softfmax) and as few as one in a binarey classifiaction model(sigmoid)\n",
        "- **Activation Function:** The function that determines the output of a nueron, such as sigmoid (0-off, 1-on), reLU, leaky reLU, tanf, and softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpGG3czIun0B"
      },
      "source": [
        "Explain how back propagation works as if you were explaining it to a five year-old. Use your own words, but feel free to reference external materials for this question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ADBYpDPyAfH"
      },
      "source": [
        "The process of sending input all the way through a neraul network, deciding how bad the resulting predicted answers made by the network are, and then going backwards through and making the adjustments that would make those predictions better, over and over again until we get predictions that are as best as we can possibly get, is called 'back propagation'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2K_00TQun0C"
      },
      "source": [
        "Remember our Simple Perceptron Class from Monday. In a simple prediction describe the process of making a prediction. How do you go from inputs to predicted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpz9Zvj3y1-e"
      },
      "source": [
        "The input is taken into the network, multiplied by the weights as determined by the activation function and sent to each connected neuron in the hiddenlayer. Those neurons sum all of the resulting values, and any bias that is required by the network architecture and pass them to the output layer determined by the activation function with that neuron's associated weights. If there are multiple nerons in the output layer, the highest score wins the prediction. if there is just one, it will result in the output either being 'on', or 'off'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsjWY3Yxun0C"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. Simple Perceptron\n",
        "\n",
        "In this question, you will build two neural networks using Tensorflow Keras. After you build these two models, compare the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYEcnlQ8un0C"
      },
      "source": [
        "\"\"\"\n",
        "Our Dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
        "                     np.linspace(-3, 3, 50))\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\"Use this X & y in the following 2 models\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIPvv8BO26Bg",
        "outputId": "71d2835b-1d8d-4d61-f674-521502ab8b3b"
      },
      "source": [
        "xx.shape, yy.shape, X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50, 50), (50, 50), (300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qNalvpV7MsU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRvKPHZoun0D"
      },
      "source": [
        "### Simple Perceptron\n",
        "Construct a simple perceptron using Keras. You model should have 1 dense layer with a single neuron and a sigmoid activation function. Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIzQQHmMun0D",
        "outputId": "89498ef2-e406-4c43-9e3b-6c9a81381bf6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "model1 = tf.keras.Sequential([Dense(1, activation='sigmoid')])\n",
        "\n",
        "model1.compile(loss='binary_crossentropy', \n",
        "              optimizer='SGD', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "h1 = model1.fit(X, y, \n",
        "          epochs=200,\n",
        "          validation_split=0.2 \n",
        "          )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8178 - accuracy: 0.4667 - val_loss: 0.7150 - val_accuracy: 0.6000\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8131 - accuracy: 0.4667 - val_loss: 0.7129 - val_accuracy: 0.6000\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8086 - accuracy: 0.4667 - val_loss: 0.7108 - val_accuracy: 0.6000\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8041 - accuracy: 0.4667 - val_loss: 0.7090 - val_accuracy: 0.6000\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8000 - accuracy: 0.4667 - val_loss: 0.7071 - val_accuracy: 0.6000\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7958 - accuracy: 0.4667 - val_loss: 0.7054 - val_accuracy: 0.5833\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7917 - accuracy: 0.4667 - val_loss: 0.7037 - val_accuracy: 0.5833\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7878 - accuracy: 0.4667 - val_loss: 0.7022 - val_accuracy: 0.5833\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7840 - accuracy: 0.4750 - val_loss: 0.7008 - val_accuracy: 0.5833\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7804 - accuracy: 0.4750 - val_loss: 0.6994 - val_accuracy: 0.5833\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7769 - accuracy: 0.4750 - val_loss: 0.6980 - val_accuracy: 0.5833\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7734 - accuracy: 0.4750 - val_loss: 0.6968 - val_accuracy: 0.5667\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7700 - accuracy: 0.4750 - val_loss: 0.6956 - val_accuracy: 0.5667\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7667 - accuracy: 0.4708 - val_loss: 0.6944 - val_accuracy: 0.5667\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7633 - accuracy: 0.4708 - val_loss: 0.6933 - val_accuracy: 0.5667\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7602 - accuracy: 0.4708 - val_loss: 0.6924 - val_accuracy: 0.5667\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7574 - accuracy: 0.4667 - val_loss: 0.6915 - val_accuracy: 0.5667\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7545 - accuracy: 0.4667 - val_loss: 0.6907 - val_accuracy: 0.5667\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7518 - accuracy: 0.4708 - val_loss: 0.6899 - val_accuracy: 0.5667\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7492 - accuracy: 0.4708 - val_loss: 0.6892 - val_accuracy: 0.5667\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7468 - accuracy: 0.4750 - val_loss: 0.6885 - val_accuracy: 0.5667\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7443 - accuracy: 0.4750 - val_loss: 0.6879 - val_accuracy: 0.5667\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7418 - accuracy: 0.4792 - val_loss: 0.6873 - val_accuracy: 0.5667\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7395 - accuracy: 0.4833 - val_loss: 0.6868 - val_accuracy: 0.5667\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7375 - accuracy: 0.4833 - val_loss: 0.6863 - val_accuracy: 0.5500\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7353 - accuracy: 0.4833 - val_loss: 0.6859 - val_accuracy: 0.5500\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7332 - accuracy: 0.4875 - val_loss: 0.6855 - val_accuracy: 0.5667\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7312 - accuracy: 0.4917 - val_loss: 0.6851 - val_accuracy: 0.5667\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7293 - accuracy: 0.4917 - val_loss: 0.6848 - val_accuracy: 0.5667\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7274 - accuracy: 0.4875 - val_loss: 0.6846 - val_accuracy: 0.5667\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7258 - accuracy: 0.4875 - val_loss: 0.6845 - val_accuracy: 0.5667\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7241 - accuracy: 0.4875 - val_loss: 0.6843 - val_accuracy: 0.5667\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7224 - accuracy: 0.4958 - val_loss: 0.6842 - val_accuracy: 0.5667\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7209 - accuracy: 0.4958 - val_loss: 0.6841 - val_accuracy: 0.5667\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7194 - accuracy: 0.5000 - val_loss: 0.6840 - val_accuracy: 0.5500\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7180 - accuracy: 0.5000 - val_loss: 0.6840 - val_accuracy: 0.5333\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7165 - accuracy: 0.5042 - val_loss: 0.6839 - val_accuracy: 0.5333\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7152 - accuracy: 0.4958 - val_loss: 0.6840 - val_accuracy: 0.5500\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7139 - accuracy: 0.5000 - val_loss: 0.6840 - val_accuracy: 0.5333\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7126 - accuracy: 0.4833 - val_loss: 0.6840 - val_accuracy: 0.5333\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7113 - accuracy: 0.4875 - val_loss: 0.6841 - val_accuracy: 0.5333\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7102 - accuracy: 0.4792 - val_loss: 0.6842 - val_accuracy: 0.5333\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7092 - accuracy: 0.4708 - val_loss: 0.6843 - val_accuracy: 0.5167\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7081 - accuracy: 0.4708 - val_loss: 0.6843 - val_accuracy: 0.5167\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7070 - accuracy: 0.4708 - val_loss: 0.6844 - val_accuracy: 0.5167\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7060 - accuracy: 0.4750 - val_loss: 0.6845 - val_accuracy: 0.5167\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7050 - accuracy: 0.4792 - val_loss: 0.6847 - val_accuracy: 0.5167\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7043 - accuracy: 0.4792 - val_loss: 0.6848 - val_accuracy: 0.5333\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7035 - accuracy: 0.4708 - val_loss: 0.6850 - val_accuracy: 0.5333\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7025 - accuracy: 0.4708 - val_loss: 0.6852 - val_accuracy: 0.5333\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7017 - accuracy: 0.4750 - val_loss: 0.6854 - val_accuracy: 0.5500\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7009 - accuracy: 0.4667 - val_loss: 0.6856 - val_accuracy: 0.5500\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7002 - accuracy: 0.4625 - val_loss: 0.6859 - val_accuracy: 0.5500\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.4667 - val_loss: 0.6861 - val_accuracy: 0.5500\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6987 - accuracy: 0.4625 - val_loss: 0.6864 - val_accuracy: 0.5500\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.4583 - val_loss: 0.6867 - val_accuracy: 0.5500\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6973 - accuracy: 0.4792 - val_loss: 0.6870 - val_accuracy: 0.5667\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6968 - accuracy: 0.4708 - val_loss: 0.6872 - val_accuracy: 0.5667\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6962 - accuracy: 0.4708 - val_loss: 0.6874 - val_accuracy: 0.5500\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.4708 - val_loss: 0.6877 - val_accuracy: 0.5500\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6951 - accuracy: 0.4625 - val_loss: 0.6879 - val_accuracy: 0.5500\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.4583 - val_loss: 0.6882 - val_accuracy: 0.5500\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6942 - accuracy: 0.4542 - val_loss: 0.6886 - val_accuracy: 0.5333\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4708 - val_loss: 0.6890 - val_accuracy: 0.5333\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.4625 - val_loss: 0.6894 - val_accuracy: 0.5500\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.4750 - val_loss: 0.6896 - val_accuracy: 0.5500\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.4708 - val_loss: 0.6899 - val_accuracy: 0.5167\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.4708 - val_loss: 0.6902 - val_accuracy: 0.5167\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.4667 - val_loss: 0.6905 - val_accuracy: 0.5167\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.4708 - val_loss: 0.6907 - val_accuracy: 0.5167\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.4500 - val_loss: 0.6910 - val_accuracy: 0.5333\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.4542 - val_loss: 0.6913 - val_accuracy: 0.5333\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.4542 - val_loss: 0.6917 - val_accuracy: 0.5333\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.4625 - val_loss: 0.6922 - val_accuracy: 0.5333\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.4917 - val_loss: 0.6925 - val_accuracy: 0.5333\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.4958 - val_loss: 0.6928 - val_accuracy: 0.5333\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.5167\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5167 - val_loss: 0.6936 - val_accuracy: 0.5500\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5375 - val_loss: 0.6938 - val_accuracy: 0.5500\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5417 - val_loss: 0.6941 - val_accuracy: 0.5667\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5458 - val_loss: 0.6943 - val_accuracy: 0.5667\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5500 - val_loss: 0.6947 - val_accuracy: 0.5667\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5542 - val_loss: 0.6949 - val_accuracy: 0.5833\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5625 - val_loss: 0.6953 - val_accuracy: 0.5833\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5875 - val_loss: 0.6956 - val_accuracy: 0.5833\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5958 - val_loss: 0.6960 - val_accuracy: 0.5833\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5958 - val_loss: 0.6962 - val_accuracy: 0.5833\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6859 - accuracy: 0.6042 - val_loss: 0.6965 - val_accuracy: 0.5833\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6858 - accuracy: 0.6000 - val_loss: 0.6968 - val_accuracy: 0.5833\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6856 - accuracy: 0.6125 - val_loss: 0.6972 - val_accuracy: 0.6000\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.6125 - val_loss: 0.6976 - val_accuracy: 0.5833\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.6208 - val_loss: 0.6980 - val_accuracy: 0.5833\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6851 - accuracy: 0.6208 - val_loss: 0.6982 - val_accuracy: 0.5833\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6849 - accuracy: 0.6375 - val_loss: 0.6986 - val_accuracy: 0.5833\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.6417 - val_loss: 0.6988 - val_accuracy: 0.5833\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.6458 - val_loss: 0.6991 - val_accuracy: 0.5833\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.6292 - val_loss: 0.6992 - val_accuracy: 0.5833\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.6375 - val_loss: 0.6995 - val_accuracy: 0.5833\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.6417 - val_loss: 0.6998 - val_accuracy: 0.5667\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6841 - accuracy: 0.6375 - val_loss: 0.7001 - val_accuracy: 0.5667\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.6375 - val_loss: 0.7004 - val_accuracy: 0.5667\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.6417 - val_loss: 0.7007 - val_accuracy: 0.5500\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.6458 - val_loss: 0.7009 - val_accuracy: 0.5500\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6838 - accuracy: 0.6500 - val_loss: 0.7013 - val_accuracy: 0.5500\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.6542 - val_loss: 0.7016 - val_accuracy: 0.5667\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6836 - accuracy: 0.6417 - val_loss: 0.7019 - val_accuracy: 0.5667\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6834 - accuracy: 0.6417 - val_loss: 0.7022 - val_accuracy: 0.5667\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.6458 - val_loss: 0.7023 - val_accuracy: 0.5667\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.6417 - val_loss: 0.7025 - val_accuracy: 0.5500\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6832 - accuracy: 0.6458 - val_loss: 0.7027 - val_accuracy: 0.5667\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6831 - accuracy: 0.6417 - val_loss: 0.7030 - val_accuracy: 0.5667\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6829 - accuracy: 0.6375 - val_loss: 0.7032 - val_accuracy: 0.5667\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.6333 - val_loss: 0.7035 - val_accuracy: 0.5667\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6829 - accuracy: 0.6333 - val_loss: 0.7038 - val_accuracy: 0.5667\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.6292 - val_loss: 0.7040 - val_accuracy: 0.5667\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6826 - accuracy: 0.6292 - val_loss: 0.7042 - val_accuracy: 0.5500\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6826 - accuracy: 0.6292 - val_loss: 0.7044 - val_accuracy: 0.5500\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.6292 - val_loss: 0.7046 - val_accuracy: 0.5500\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.6292 - val_loss: 0.7047 - val_accuracy: 0.5500\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6825 - accuracy: 0.6292 - val_loss: 0.7048 - val_accuracy: 0.5500\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6824 - accuracy: 0.6292 - val_loss: 0.7050 - val_accuracy: 0.5500\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.6250 - val_loss: 0.7052 - val_accuracy: 0.5333\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6824 - accuracy: 0.6333 - val_loss: 0.7055 - val_accuracy: 0.5333\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6822 - accuracy: 0.6292 - val_loss: 0.7056 - val_accuracy: 0.5333\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6822 - accuracy: 0.6292 - val_loss: 0.7059 - val_accuracy: 0.5333\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.6333 - val_loss: 0.7061 - val_accuracy: 0.5333\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6821 - accuracy: 0.6333 - val_loss: 0.7063 - val_accuracy: 0.5333\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6820 - accuracy: 0.6333 - val_loss: 0.7066 - val_accuracy: 0.5167\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6820 - accuracy: 0.6333 - val_loss: 0.7066 - val_accuracy: 0.5167\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6820 - accuracy: 0.6333 - val_loss: 0.7069 - val_accuracy: 0.5167\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.6333 - val_loss: 0.7071 - val_accuracy: 0.5167\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.6333 - val_loss: 0.7073 - val_accuracy: 0.5000\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.6375 - val_loss: 0.7076 - val_accuracy: 0.5000\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7078 - val_accuracy: 0.5000\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7079 - val_accuracy: 0.5000\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7082 - val_accuracy: 0.5000\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.6375 - val_loss: 0.7083 - val_accuracy: 0.5000\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.6375 - val_loss: 0.7085 - val_accuracy: 0.5000\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6817 - accuracy: 0.6417 - val_loss: 0.7087 - val_accuracy: 0.5000\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6817 - accuracy: 0.6375 - val_loss: 0.7088 - val_accuracy: 0.5000\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.6375 - val_loss: 0.7089 - val_accuracy: 0.5000\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.6375 - val_loss: 0.7092 - val_accuracy: 0.5000\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.6417 - val_loss: 0.7092 - val_accuracy: 0.5000\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.6333 - val_loss: 0.7093 - val_accuracy: 0.5000\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.6375 - val_loss: 0.7093 - val_accuracy: 0.5000\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.6375 - val_loss: 0.7095 - val_accuracy: 0.5000\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6815 - accuracy: 0.6417 - val_loss: 0.7096 - val_accuracy: 0.5000\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.6375 - val_loss: 0.7097 - val_accuracy: 0.5000\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.6417 - val_loss: 0.7097 - val_accuracy: 0.5000\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.6417 - val_loss: 0.7097 - val_accuracy: 0.5000\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.6375 - val_loss: 0.7099 - val_accuracy: 0.5000\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.6458 - val_loss: 0.7100 - val_accuracy: 0.5000\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.6417 - val_loss: 0.7102 - val_accuracy: 0.5000\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.6458 - val_loss: 0.7104 - val_accuracy: 0.5000\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.6458 - val_loss: 0.7105 - val_accuracy: 0.5000\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.6458 - val_loss: 0.7107 - val_accuracy: 0.5000\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6458 - val_loss: 0.7108 - val_accuracy: 0.5000\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.6458 - val_loss: 0.7109 - val_accuracy: 0.5000\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6458 - val_loss: 0.7111 - val_accuracy: 0.5000\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6458 - val_loss: 0.7111 - val_accuracy: 0.5000\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6417 - val_loss: 0.7113 - val_accuracy: 0.5000\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.6417 - val_loss: 0.7113 - val_accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6417 - val_loss: 0.7113 - val_accuracy: 0.5000\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6458 - val_loss: 0.7114 - val_accuracy: 0.5000\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6417 - val_loss: 0.7115 - val_accuracy: 0.5000\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6417 - val_loss: 0.7114 - val_accuracy: 0.5000\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6458 - val_loss: 0.7115 - val_accuracy: 0.5000\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6500 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6813 - accuracy: 0.6417 - val_loss: 0.7118 - val_accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6813 - accuracy: 0.6500 - val_loss: 0.7116 - val_accuracy: 0.5000\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7115 - val_accuracy: 0.5000\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.6500 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7119 - val_accuracy: 0.5000\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7120 - val_accuracy: 0.5000\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7122 - val_accuracy: 0.5000\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7123 - val_accuracy: 0.5000\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6500 - val_loss: 0.7122 - val_accuracy: 0.5000\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7125 - val_accuracy: 0.5000\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7125 - val_accuracy: 0.5000\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7125 - val_accuracy: 0.5000\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7126 - val_accuracy: 0.5000\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7126 - val_accuracy: 0.5000\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7128 - val_accuracy: 0.5000\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7129 - val_accuracy: 0.5000\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7129 - val_accuracy: 0.5000\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7129 - val_accuracy: 0.5000\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7131 - val_accuracy: 0.5000\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.6458 - val_loss: 0.7130 - val_accuracy: 0.4833\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.6500 - val_loss: 0.7130 - val_accuracy: 0.4833\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.6458 - val_loss: 0.7131 - val_accuracy: 0.5000\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6458 - val_loss: 0.7131 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnJnP3Erun0D"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "Now construct a multi-layer perceptron using. Here are some architecture suggestions: \n",
        "- 2 Hidden Layers\n",
        "- 5-32 Neurons in the Hidden Layers\n",
        "- Your pick of activation function and optimizer\n",
        "- Incorporate the Callback function below into your model\n",
        "\n",
        "Your model should be called `model2` and make sure to save the results of your fit statement to a variable called `h2`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LavceiIAun0D"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        if(logs.get('accuracy') > .99999):   \n",
        "            self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzt_Uabu5OxV",
        "outputId": "aa09e08b-0cf1-45cc-9112-24b7cbb8bf00"
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "      Dense(32, activation='relu'),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "h2 = model2.fit(X, y, \n",
        "          epochs=1111, \n",
        "          validation_split=0.2,\n",
        "          callbacks=[myCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1111\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6726 - accuracy: 0.6458 - val_loss: 0.6621 - val_accuracy: 0.7167\n",
            "Epoch 2/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.7708 - val_loss: 0.6372 - val_accuracy: 0.8667\n",
            "Epoch 3/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6190 - accuracy: 0.8792 - val_loss: 0.6166 - val_accuracy: 0.8833\n",
            "Epoch 4/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.9042 - val_loss: 0.5970 - val_accuracy: 0.9000\n",
            "Epoch 5/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5772 - accuracy: 0.9167 - val_loss: 0.5786 - val_accuracy: 0.9333\n",
            "Epoch 6/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5580 - accuracy: 0.9208 - val_loss: 0.5616 - val_accuracy: 0.9333\n",
            "Epoch 7/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.9417 - val_loss: 0.5458 - val_accuracy: 0.9333\n",
            "Epoch 8/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5210 - accuracy: 0.9458 - val_loss: 0.5300 - val_accuracy: 0.9333\n",
            "Epoch 9/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5029 - accuracy: 0.9625 - val_loss: 0.5135 - val_accuracy: 0.9167\n",
            "Epoch 10/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4842 - accuracy: 0.9792 - val_loss: 0.4967 - val_accuracy: 0.9167\n",
            "Epoch 11/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4660 - accuracy: 0.9708 - val_loss: 0.4799 - val_accuracy: 0.9167\n",
            "Epoch 12/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.9750 - val_loss: 0.4622 - val_accuracy: 0.9333\n",
            "Epoch 13/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.9708 - val_loss: 0.4448 - val_accuracy: 0.9333\n",
            "Epoch 14/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.9750 - val_loss: 0.4287 - val_accuracy: 0.9333\n",
            "Epoch 15/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3944 - accuracy: 0.9792 - val_loss: 0.4121 - val_accuracy: 0.9500\n",
            "Epoch 16/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.9792 - val_loss: 0.3968 - val_accuracy: 0.9500\n",
            "Epoch 17/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3614 - accuracy: 0.9792 - val_loss: 0.3804 - val_accuracy: 0.9500\n",
            "Epoch 18/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3460 - accuracy: 0.9792 - val_loss: 0.3653 - val_accuracy: 0.9500\n",
            "Epoch 19/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.9792 - val_loss: 0.3500 - val_accuracy: 0.9500\n",
            "Epoch 20/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.9792 - val_loss: 0.3376 - val_accuracy: 0.9500\n",
            "Epoch 21/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3037 - accuracy: 0.9792 - val_loss: 0.3234 - val_accuracy: 0.9500\n",
            "Epoch 22/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2906 - accuracy: 0.9792 - val_loss: 0.3101 - val_accuracy: 0.9500\n",
            "Epoch 23/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2785 - accuracy: 0.9833 - val_loss: 0.2974 - val_accuracy: 0.9500\n",
            "Epoch 24/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.9833 - val_loss: 0.2864 - val_accuracy: 0.9500\n",
            "Epoch 25/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2566 - accuracy: 0.9833 - val_loss: 0.2765 - val_accuracy: 0.9667\n",
            "Epoch 26/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2467 - accuracy: 0.9792 - val_loss: 0.2674 - val_accuracy: 0.9667\n",
            "Epoch 27/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9792 - val_loss: 0.2577 - val_accuracy: 0.9500\n",
            "Epoch 28/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9792 - val_loss: 0.2474 - val_accuracy: 0.9833\n",
            "Epoch 29/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2201 - accuracy: 0.9792 - val_loss: 0.2392 - val_accuracy: 0.9833\n",
            "Epoch 30/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2122 - accuracy: 0.9792 - val_loss: 0.2283 - val_accuracy: 0.9833\n",
            "Epoch 31/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2054 - accuracy: 0.9833 - val_loss: 0.2211 - val_accuracy: 0.9833\n",
            "Epoch 32/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9833 - val_loss: 0.2141 - val_accuracy: 0.9833\n",
            "Epoch 33/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1918 - accuracy: 0.9833 - val_loss: 0.2095 - val_accuracy: 0.9833\n",
            "Epoch 34/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.9833 - val_loss: 0.2052 - val_accuracy: 0.9667\n",
            "Epoch 35/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9792 - val_loss: 0.1997 - val_accuracy: 0.9833\n",
            "Epoch 36/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9792 - val_loss: 0.1921 - val_accuracy: 0.9833\n",
            "Epoch 37/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9833 - val_loss: 0.1877 - val_accuracy: 0.9833\n",
            "Epoch 38/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.9750 - val_loss: 0.1827 - val_accuracy: 0.9833\n",
            "Epoch 39/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9792 - val_loss: 0.1782 - val_accuracy: 0.9833\n",
            "Epoch 40/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.9833 - val_loss: 0.1738 - val_accuracy: 0.9667\n",
            "Epoch 41/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.9833 - val_loss: 0.1678 - val_accuracy: 0.9833\n",
            "Epoch 42/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9833 - val_loss: 0.1651 - val_accuracy: 0.9833\n",
            "Epoch 43/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.9833 - val_loss: 0.1624 - val_accuracy: 0.9833\n",
            "Epoch 44/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9833 - val_loss: 0.1584 - val_accuracy: 0.9833\n",
            "Epoch 45/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9875 - val_loss: 0.1546 - val_accuracy: 0.9833\n",
            "Epoch 46/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9833 - val_loss: 0.1488 - val_accuracy: 0.9833\n",
            "Epoch 47/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9875 - val_loss: 0.1471 - val_accuracy: 0.9833\n",
            "Epoch 48/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9875 - val_loss: 0.1452 - val_accuracy: 0.9500\n",
            "Epoch 49/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1253 - accuracy: 0.9875 - val_loss: 0.1430 - val_accuracy: 0.9667\n",
            "Epoch 50/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1229 - accuracy: 0.9875 - val_loss: 0.1425 - val_accuracy: 0.9667\n",
            "Epoch 51/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.1203 - accuracy: 0.9833 - val_loss: 0.1382 - val_accuracy: 0.9833\n",
            "Epoch 52/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9875 - val_loss: 0.1361 - val_accuracy: 0.9667\n",
            "Epoch 53/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9875 - val_loss: 0.1342 - val_accuracy: 0.9667\n",
            "Epoch 54/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1133 - accuracy: 0.9875 - val_loss: 0.1328 - val_accuracy: 0.9667\n",
            "Epoch 55/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1107 - accuracy: 0.9875 - val_loss: 0.1306 - val_accuracy: 0.9500\n",
            "Epoch 56/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9875 - val_loss: 0.1291 - val_accuracy: 0.9500\n",
            "Epoch 57/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9875 - val_loss: 0.1252 - val_accuracy: 0.9667\n",
            "Epoch 58/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9875 - val_loss: 0.1233 - val_accuracy: 0.9833\n",
            "Epoch 59/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9875 - val_loss: 0.1210 - val_accuracy: 0.9833\n",
            "Epoch 60/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1011 - accuracy: 0.9875 - val_loss: 0.1200 - val_accuracy: 0.9667\n",
            "Epoch 61/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9875 - val_loss: 0.1178 - val_accuracy: 0.9833\n",
            "Epoch 62/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9875 - val_loss: 0.1165 - val_accuracy: 0.9500\n",
            "Epoch 63/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9875 - val_loss: 0.1151 - val_accuracy: 0.9667\n",
            "Epoch 64/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9875 - val_loss: 0.1149 - val_accuracy: 0.9500\n",
            "Epoch 65/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9875 - val_loss: 0.1105 - val_accuracy: 0.9833\n",
            "Epoch 66/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9875 - val_loss: 0.1088 - val_accuracy: 0.9833\n",
            "Epoch 67/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9875 - val_loss: 0.1073 - val_accuracy: 0.9833\n",
            "Epoch 68/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.9875 - val_loss: 0.1073 - val_accuracy: 0.9833\n",
            "Epoch 69/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9917 - val_loss: 0.1067 - val_accuracy: 0.9833\n",
            "Epoch 70/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9917 - val_loss: 0.1064 - val_accuracy: 0.9833\n",
            "Epoch 71/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9875 - val_loss: 0.1046 - val_accuracy: 0.9667\n",
            "Epoch 72/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9875 - val_loss: 0.1018 - val_accuracy: 0.9667\n",
            "Epoch 73/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9917 - val_loss: 0.0998 - val_accuracy: 0.9833\n",
            "Epoch 74/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9917 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
            "Epoch 75/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9917 - val_loss: 0.0976 - val_accuracy: 0.9833\n",
            "Epoch 76/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9917 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
            "Epoch 77/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9917 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
            "Epoch 78/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9917 - val_loss: 0.0964 - val_accuracy: 0.9833\n",
            "Epoch 79/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9958 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
            "Epoch 80/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9958 - val_loss: 0.0927 - val_accuracy: 0.9833\n",
            "Epoch 81/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9917 - val_loss: 0.0933 - val_accuracy: 0.9833\n",
            "Epoch 82/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9875 - val_loss: 0.0948 - val_accuracy: 0.9833\n",
            "Epoch 83/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9917 - val_loss: 0.0920 - val_accuracy: 0.9833\n",
            "Epoch 84/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.9917 - val_loss: 0.0912 - val_accuracy: 1.0000\n",
            "Epoch 85/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9917 - val_loss: 0.0913 - val_accuracy: 0.9833\n",
            "Epoch 86/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9917 - val_loss: 0.0903 - val_accuracy: 0.9667\n",
            "Epoch 87/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9917 - val_loss: 0.0897 - val_accuracy: 0.9667\n",
            "Epoch 88/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9917 - val_loss: 0.0890 - val_accuracy: 0.9833\n",
            "Epoch 89/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9917 - val_loss: 0.0883 - val_accuracy: 0.9833\n",
            "Epoch 90/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9917 - val_loss: 0.0849 - val_accuracy: 0.9833\n",
            "Epoch 91/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9958 - val_loss: 0.0836 - val_accuracy: 0.9833\n",
            "Epoch 92/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9958 - val_loss: 0.0832 - val_accuracy: 0.9833\n",
            "Epoch 93/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9958 - val_loss: 0.0838 - val_accuracy: 0.9833\n",
            "Epoch 94/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9958 - val_loss: 0.0828 - val_accuracy: 0.9833\n",
            "Epoch 95/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9917 - val_loss: 0.0829 - val_accuracy: 0.9833\n",
            "Epoch 96/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9917 - val_loss: 0.0837 - val_accuracy: 0.9833\n",
            "Epoch 97/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9917 - val_loss: 0.0820 - val_accuracy: 0.9833\n",
            "Epoch 98/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9917 - val_loss: 0.0826 - val_accuracy: 0.9833\n",
            "Epoch 99/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9917 - val_loss: 0.0775 - val_accuracy: 0.9833\n",
            "Epoch 100/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9958 - val_loss: 0.0765 - val_accuracy: 0.9833\n",
            "Epoch 101/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9958 - val_loss: 0.0772 - val_accuracy: 0.9833\n",
            "Epoch 102/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9958 - val_loss: 0.0776 - val_accuracy: 0.9833\n",
            "Epoch 103/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0576 - accuracy: 0.9917 - val_loss: 0.0790 - val_accuracy: 0.9833\n",
            "Epoch 104/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.9958 - val_loss: 0.0769 - val_accuracy: 0.9833\n",
            "Epoch 105/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9958 - val_loss: 0.0756 - val_accuracy: 0.9833\n",
            "Epoch 106/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0558 - accuracy: 0.9958 - val_loss: 0.0753 - val_accuracy: 0.9833\n",
            "Epoch 107/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9958 - val_loss: 0.0754 - val_accuracy: 0.9833\n",
            "Epoch 108/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0548 - accuracy: 0.9958 - val_loss: 0.0751 - val_accuracy: 0.9833\n",
            "Epoch 109/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9958 - val_loss: 0.0745 - val_accuracy: 0.9833\n",
            "Epoch 110/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9917 - val_loss: 0.0738 - val_accuracy: 0.9833\n",
            "Epoch 111/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 0.9917 - val_loss: 0.0731 - val_accuracy: 0.9833\n",
            "Epoch 112/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9917 - val_loss: 0.0740 - val_accuracy: 0.9833\n",
            "Epoch 113/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9917 - val_loss: 0.0730 - val_accuracy: 0.9833\n",
            "Epoch 114/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9958 - val_loss: 0.0717 - val_accuracy: 0.9833\n",
            "Epoch 115/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9958 - val_loss: 0.0704 - val_accuracy: 0.9833\n",
            "Epoch 116/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9958 - val_loss: 0.0690 - val_accuracy: 0.9833\n",
            "Epoch 117/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0504 - accuracy: 0.9958 - val_loss: 0.0696 - val_accuracy: 0.9833\n",
            "Epoch 118/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9958 - val_loss: 0.0697 - val_accuracy: 0.9833\n",
            "Epoch 119/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0494 - accuracy: 0.9958 - val_loss: 0.0683 - val_accuracy: 0.9833\n",
            "Epoch 120/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.9917 - val_loss: 0.0698 - val_accuracy: 0.9833\n",
            "Epoch 121/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.9958 - val_loss: 0.0682 - val_accuracy: 0.9833\n",
            "Epoch 122/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.9958 - val_loss: 0.0690 - val_accuracy: 0.9833\n",
            "Epoch 123/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9958 - val_loss: 0.0667 - val_accuracy: 0.9833\n",
            "Epoch 124/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9917 - val_loss: 0.0658 - val_accuracy: 0.9833\n",
            "Epoch 125/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9958 - val_loss: 0.0668 - val_accuracy: 0.9833\n",
            "Epoch 126/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0464 - accuracy: 0.9958 - val_loss: 0.0648 - val_accuracy: 0.9833\n",
            "Epoch 127/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9958 - val_loss: 0.0670 - val_accuracy: 0.9833\n",
            "Epoch 128/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 0.9958 - val_loss: 0.0646 - val_accuracy: 0.9833\n",
            "Epoch 129/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9958 - val_loss: 0.0653 - val_accuracy: 0.9833\n",
            "Epoch 130/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9958 - val_loss: 0.0655 - val_accuracy: 0.9833\n",
            "Epoch 131/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9958 - val_loss: 0.0647 - val_accuracy: 0.9833\n",
            "Epoch 132/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9958 - val_loss: 0.0640 - val_accuracy: 0.9833\n",
            "Epoch 133/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9958 - val_loss: 0.0634 - val_accuracy: 0.9833\n",
            "Epoch 134/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9958 - val_loss: 0.0617 - val_accuracy: 0.9833\n",
            "Epoch 135/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.9958 - val_loss: 0.0622 - val_accuracy: 0.9833\n",
            "Epoch 136/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9958 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
            "Epoch 137/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9958 - val_loss: 0.0598 - val_accuracy: 0.9833\n",
            "Epoch 138/1111\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0421 - accuracy: 0.9958 - val_loss: 0.0592 - val_accuracy: 0.9833\n",
            "Epoch 139/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 0.9958 - val_loss: 0.0603 - val_accuracy: 0.9833\n",
            "Epoch 140/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.9958 - val_loss: 0.0595 - val_accuracy: 0.9833\n",
            "Epoch 141/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.9958 - val_loss: 0.0603 - val_accuracy: 0.9833\n",
            "Epoch 142/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9958 - val_loss: 0.0593 - val_accuracy: 0.9833\n",
            "Epoch 143/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9958 - val_loss: 0.0581 - val_accuracy: 0.9833\n",
            "Epoch 144/1111\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0401 - accuracy: 0.9958 - val_loss: 0.0578 - val_accuracy: 0.9833\n",
            "Epoch 145/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9958 - val_loss: 0.0588 - val_accuracy: 0.9833\n",
            "Epoch 146/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9958 - val_loss: 0.0597 - val_accuracy: 0.9833\n",
            "Epoch 147/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.9958 - val_loss: 0.0599 - val_accuracy: 0.9833\n",
            "Epoch 148/1111\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0WUdLc_un0D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuTttxAIun0D"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "**Before you Start**: You will need to install an additional library for this next segment. Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
        "\n",
        "\n",
        "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "123DY13sun0D",
        "outputId": "70605031-1b2e-49fe-a4e5-ace6ced043f4"
      },
      "source": [
        "# Do Not change anything in this cell\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
        "y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nul93Dluun0D"
      },
      "source": [
        "Why does the Perceptron (`model1`) only achieve ~70% accuracy? What is the architectural property of the Multi-Layer Perceptron that allows it more accurately learn the relationship between X and y? \n",
        "\n",
        "Why might this property be useful in more complex data such as images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Ne1YtgGJjt"
      },
      "source": [
        "In model1 the data can only be split into on one category or the other by one calculation. There is no single linear separation of the data that can possibly contain only the correct labels. the model does its best to get the least amount of loss (the fewest errors) but a single sigmoid function cannot classify the data at a higher rate given this dataset.\n",
        "\n",
        "With multiple layers, the second model does a perfect job of determining wich class the data should fall into. \n",
        "\n",
        "If (x < 0 AND y < 0) OR (x > 0 AND y > 0), the point usually fits into category 0. \n",
        "\n",
        "If (x < 0 AND y > 0) OR (x > 0 AND y < 0), the point usually fits into category 1. \n",
        "\n",
        "Non-linear separation would be useful in image recognition with even more layers because the color, saturation, position, and relational postion of each pixel could all determine wether a colection of pixels represents a certain type of image or not. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6UYcZbsun0D"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "igtL6OWuun0D",
        "outputId": "737dc8f8-c5b4-4c1a-ac26-b5c2e1f4901c"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>147</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>142</td>\n",
              "      <td>309</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "      <td>164</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>6.2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>112</td>\n",
              "      <td>230</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "97    52    1   0       108   233    1  ...      0      0.1      2   3     3       1\n",
              "295   63    1   0       140   187    0  ...      1      4.0      2   2     3       0\n",
              "255   45    1   0       142   309    0  ...      1      0.0      1   3     3       0\n",
              "204   62    0   0       160   164    0  ...      0      6.2      0   3     3       0\n",
              "183   58    1   2       112   230    0  ...      0      2.5      1   1     3       0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Suus0pZOrfr"
      },
      "source": [
        "#Splitting the data\n",
        "X = df[df.columns.drop('target')]\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20,\n",
        "    stratify= y,\n",
        "    random_state=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "kJif3Lz6P8nY",
        "outputId": "9d3e177e-180f-4d13-e4d5-1a8ead362aef"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>112</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>143</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>160</td>\n",
              "      <td>273</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>247</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>143</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  ...  exang  oldpeak  slope  ca  thal\n",
              "19    69    0   3       140   239  ...      0      1.8      2   2     2\n",
              "126   47    1   0       112   204  ...      0      0.1      2   0     2\n",
              "17    66    0   3       150   226  ...      0      2.6      0   0     2\n",
              "254   59    1   3       160   273  ...      0      0.0      2   0     2\n",
              "251   43    1   0       132   247  ...      1      0.1      1   4     3\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSdcoZ3RQHyv",
        "outputId": "a4250671-48ab-4580-c454-b70e84677346"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19     1\n",
              "126    1\n",
              "17     1\n",
              "254    0\n",
              "251    0\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4epBwN3_QKp5",
        "outputId": "f1ee94c3-fb44-4967-9049-7a26a2c0ec5f"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((242, 13), (242,), (61, 13), (61,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPbaPRQbMbdu"
      },
      "source": [
        "#scaling the data\n",
        "scaler = StandardScaler()\n",
        "X_train_fit = scaler.fit_transform(X_train)\n",
        "X_test_fit = scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4lKl26jMj0E",
        "outputId": "9db4ebab-dbb8-4565-c89a-10c981972872"
      },
      "source": [
        "X_test_fit[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.46693688,  0.54577682,  1.7631813 , -0.73567087, -0.14419176,\n",
              "        -0.29880715,  0.95197164,  1.35723445,  1.67705098,  2.46894231,\n",
              "        -0.75349652, -0.60662331,  1.05045146],\n",
              "       [-0.0110158 , -1.83225076,  0.86689747, -0.73567087,  1.22213018,\n",
              "        -0.29880715, -1.05045146,  0.23964129, -0.59628479, -0.32166301,\n",
              "         0.94884747, -0.60662331, -0.55148702],\n",
              "       [ 0.77294171,  0.54577682, -0.92567018, -1.95182056, -0.08014542,\n",
              "        -0.29880715,  0.95197164,  0.19493756, -0.59628479, -0.75769509,\n",
              "         0.94884747,  0.36716674,  1.05045146],\n",
              "       [ 0.88493564,  0.54577682,  1.7631813 ,  2.30470334,  1.07268872,\n",
              "        -0.29880715, -1.05045146,  0.32904874, -0.59628479, -0.67048868,\n",
              "        -0.75349652, -0.60662331,  1.05045146],\n",
              "       [-1.9149126 ,  0.54577682,  1.7631813 , -0.85728584, -1.190282  ,\n",
              "        -0.29880715, -1.05045146,  0.99960464, -0.59628479, -0.84490151,\n",
              "         0.94884747, -0.60662331, -0.55148702]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyG8fv33un0F"
      },
      "source": [
        "#create the model\n",
        "tf.random.set_seed(17)\n",
        "logdir = os.path.join(\"logs\", \"EarlyStopping-Loss\")\n",
        "\n",
        "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_dim= 13),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acKXjC6ORRif",
        "outputId": "f991c804-4da2-4b15-9042-e875a99a454c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_88 (Dense)             (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 12,161\n",
            "Trainable params: 12,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etK-iW44S8zt",
        "outputId": "6689e951-58c1-4d58-f705-22c9a2063634"
      },
      "source": [
        "#baseline fit\n",
        "model.fit(X_train_fit, y_train, epochs=72, \n",
        "          validation_data=(X_test_fit,y_test),\n",
        "          callbacks=[myCallback(), stop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/72\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6477 - accuracy: 0.6653 - val_loss: 0.5957 - val_accuracy: 0.8197\n",
            "Epoch 2/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5301 - accuracy: 0.8140 - val_loss: 0.5077 - val_accuracy: 0.8197\n",
            "Epoch 3/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.8388 - val_loss: 0.4375 - val_accuracy: 0.8033\n",
            "Epoch 4/72\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3752 - accuracy: 0.8554 - val_loss: 0.4044 - val_accuracy: 0.8197\n",
            "Epoch 5/72\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8636 - val_loss: 0.3955 - val_accuracy: 0.8033\n",
            "Epoch 6/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2996 - accuracy: 0.8802 - val_loss: 0.4030 - val_accuracy: 0.8197\n",
            "Epoch 7/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2785 - accuracy: 0.8967 - val_loss: 0.4132 - val_accuracy: 0.8033\n",
            "Epoch 8/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.8884 - val_loss: 0.4283 - val_accuracy: 0.8197\n",
            "Epoch 9/72\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2465 - accuracy: 0.8926 - val_loss: 0.4311 - val_accuracy: 0.8033\n",
            "Epoch 10/72\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2333 - accuracy: 0.9008 - val_loss: 0.4382 - val_accuracy: 0.8033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f322ce4a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEz0ISfoRVh3",
        "outputId": "e6cb7062-271e-4319-de5e-785f1bcca7d6"
      },
      "source": [
        "model.evaluate(X_train_fit, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22221839427947998, 0.9049586653709412]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRfdUsIiYlTY"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(units=16):\n",
        "    # create model\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(units, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCh-yu_pYeOx"
      },
      "source": [
        "model_g = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIJn_2-HRUar"
      },
      "source": [
        "param_grid = {'batch_size': [32],\n",
        "              'epochs': [20],\n",
        "              'units':[32, 64, 128]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AmgjOQJVuUA",
        "outputId": "c7d41601-a258-4f52-9d44-f28d4831b622"
      },
      "source": [
        "grid = GridSearchCV(estimator=model_g, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_train_fit, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6771 - accuracy: 0.5544\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6447 - accuracy: 0.5907\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.6425\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.7306\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7876\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7824\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.7824\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5186 - accuracy: 0.7824\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.7979\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8083\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.8083\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8135\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8135\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8238\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8238\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8290\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8290\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8394\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8342\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8394\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.8980\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6759 - accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.6321\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5750 - accuracy: 0.7409\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7927\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5024 - accuracy: 0.8031\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4695 - accuracy: 0.8135\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8187\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8394\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8497\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8653\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8756\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8653\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8705\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8860\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8756\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8860\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.8912\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2816 - accuracy: 0.8964\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8964\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8964\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.8367\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6495\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.7680\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7835\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7835\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5098 - accuracy: 0.7938\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8093\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8299\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8557\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8557\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3891 - accuracy: 0.8557\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8557\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.8557\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8608\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8505\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8711\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.8660\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.8763\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3072 - accuracy: 0.8711\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8763\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2985 - accuracy: 0.8763\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8542\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.4845\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6289\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.7371\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5533 - accuracy: 0.7938\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5146 - accuracy: 0.8247\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4736 - accuracy: 0.8351\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.8711\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3872 - accuracy: 0.8814\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8814\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8866\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8918\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.9021\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.9227\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.9175\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.9175\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.9175\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9278\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9330\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9330\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9330\n",
            "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3220af9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.7500\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6514 - accuracy: 0.6701\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.7165\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5674 - accuracy: 0.7423\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5358 - accuracy: 0.7474\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5105 - accuracy: 0.7474\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.7732\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.7835\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4387 - accuracy: 0.8093\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4189 - accuracy: 0.8093\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8144\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8144\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8247\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3553 - accuracy: 0.8299\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8351\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8351\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8402\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8505\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8454\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3077 - accuracy: 0.8557\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8608\n",
            "WARNING:tensorflow:6 out of the last 25 calls to <function Model.make_test_function.<locals>.test_function at 0x7f322b2e48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.8333\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6444 - accuracy: 0.6995\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7927\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.8187\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8187\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4132 - accuracy: 0.8290\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8446\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8601\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8497\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8497\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8446\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8601\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8549\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.8549\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8653\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8653\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8705\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8705\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8808\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8860\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2731 - accuracy: 0.8860\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f321a974400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.1891 - accuracy: 0.9388\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6358 - accuracy: 0.5803\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7358\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8135\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8394\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8549\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8497\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8549\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8653\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8860\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8808\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2581 - accuracy: 0.9016\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8964\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.9119\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9067\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9067\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2207 - accuracy: 0.9326\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9482\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9378\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9378\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9378\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f322a9b4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8571\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.6031\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7423\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7835\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7938\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.8247\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8454\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8454\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8505\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8608\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8660\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8711\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8763\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8711\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2841 - accuracy: 0.8763\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.9072\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8866\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8969\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.9175\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.9072\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9124\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3216ba8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.3102 - accuracy: 0.8750\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.7163 - accuracy: 0.4330\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6181 - accuracy: 0.7371\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5378 - accuracy: 0.8711\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.8608\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.8608\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.3370 - accuracy: 0.8814\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.9124\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.9124\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9175\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9278\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9227\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9381\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9227\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9278\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9381\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9381\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9485\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1560 - accuracy: 0.9536\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.9588\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9588\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3216a28158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.7112 - accuracy: 0.7708\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6573 - accuracy: 0.6031\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7680\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7938\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.8144\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8402\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8660\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8608\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8557\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8763\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8918\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.9021\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.9021\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.9072\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2454 - accuracy: 0.9021\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.9072\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.9175\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9330\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9381\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1978 - accuracy: 0.9330\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9330\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f321693c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8958\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6736\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5134 - accuracy: 0.7824\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8031\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8238\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8549\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8653\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3097 - accuracy: 0.8756\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8601\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8705\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8808\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2460 - accuracy: 0.8964\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.8964\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2186 - accuracy: 0.9067\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9223\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9275\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9326\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.9326\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9482\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.9378\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9482\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3216c0fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.8776\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.7098\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.8446\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8601\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8653\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8756\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.9171\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9223\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9223\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9430\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9585\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.9585\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9689\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9741\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9689\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9637\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9689\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9741\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.9741\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9793\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9845\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f321657da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7959\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.6753\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4654 - accuracy: 0.8557\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8608\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8660\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.8608\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8711\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8866\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8918\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2443 - accuracy: 0.9124\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9124\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9175\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9278\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9381\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1877 - accuracy: 0.9330\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9485\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9381\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9639\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.9433\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1313 - accuracy: 0.9588\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.9588\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f32023d39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8958\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6082\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.8918\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8763\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8711\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8711\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2646 - accuracy: 0.9021\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.9021\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2141 - accuracy: 0.9227\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9175\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.9330\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9381\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9536\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9381\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9433\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9485\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9691\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9639\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9691\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9794\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9845\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f322cd0b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.9914 - accuracy: 0.7708\n",
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6194 - accuracy: 0.7216\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8660\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8608\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8608\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8557\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8918\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8660\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8918\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2445 - accuracy: 0.8866\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.8969\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.9175\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1985 - accuracy: 0.9278\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9278\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9278\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9330\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9536\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9639\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9691\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9639\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1103 - accuracy: 0.9588\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f3280584488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3582 - accuracy: 0.8542\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6490 - accuracy: 0.6281\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7810\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.8223\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4093 - accuracy: 0.8430\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8595\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8719\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8843\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2697 - accuracy: 0.8802\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.8802\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.9008\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.9215\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2188 - accuracy: 0.9298\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9256\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9298\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9380\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9421\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9421\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1529 - accuracy: 0.9504\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1417 - accuracy: 0.9587\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mu92AfEae2u",
        "outputId": "f49157a9-56d8-4643-f2b1-8b18998f084b"
      },
      "source": [
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8675170063972473 using {'batch_size': 32, 'epochs': 20, 'units': 64}\n",
            "Means: 0.8344387769699096, Stdev: 0.04808415854881698 with: {'batch_size': 32, 'epochs': 20, 'units': 32}\n",
            "Means: 0.8675170063972473, Stdev: 0.0554814315600159 with: {'batch_size': 32, 'epochs': 20, 'units': 64}\n",
            "Means: 0.8388605356216431, Stdev: 0.04785157017449535 with: {'batch_size': 32, 'epochs': 20, 'units': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}