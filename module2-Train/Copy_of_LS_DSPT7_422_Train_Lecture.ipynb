{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LS_DSPT7_422_Train_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS4GZ37Wgcjr"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 2, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etFf1WLWgcjt",
        "toc-hr-collapsed": false
      },
      "source": [
        "# Train (Prepare)\n",
        "__*Neural Network Foundations*__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXB80QOhgcju"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Student should be able to explain the intuition behind backpropagation and gradient descent\n",
        "* <a href=\"#p2\">Part 2</a>: Student should be able to discuss the importance of batch size\n",
        "* <a href=\"#p3\">Part 3</a>: Student should be able to discuss the importance of learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YuQu2lfgcju"
      },
      "source": [
        "## Summary of Yesterday\n",
        "\n",
        "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
        "\n",
        "  <center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
        "\n",
        "- There are three kinds of layers: input, hidden, and output layers.\n",
        "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
        "- Signal is passed from layer to layer through a network by:\n",
        " - Taking in inputs from the training data (or previous layer)\n",
        " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
        " - Adding a bias to this weighted some of inputs and weights\n",
        " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
        "\\begin{align}\n",
        " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
        "\\end{align}\n",
        " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi4R03rgcjv"
      },
      "source": [
        "## Training a Neural Network: *Formal Summary*\n",
        "\n",
        "0. Pick a network architecture\n",
        "   - No. of input units = No. of features\n",
        "   - No. of output units = Number of Classes (or expected targets)\n",
        "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
        "1. Randomly initialize weights\n",
        "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
        "3. Implement code to compute a cost function $J(\\theta)$\n",
        "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
        "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
        "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4CK1IarId4",
        "toc-hr-collapsed": false
      },
      "source": [
        "# Backpropagation & Gradient Descent (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktm8Fmoagcjy",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview\n",
        "\n",
        "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXI2tEO9gcjy"
      },
      "source": [
        "### What is a Gradient?\n",
        "\n",
        "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
        "\n",
        "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZY66kiUgcjz",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "In this section, we will again a simple neural network using base TensorFlow. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm2HPETcrgy6",
        "toc-hr-collapsed": true
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_99C94113202D-1.jpeg\"width=500></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4tzpwO6B47"
      },
      "source": [
        "### Generate some Fake Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERyVgeO_IWyV"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Imagine that our data is drawn from a linear function\n",
        "# y = 3*hours_studying + 50\n",
        "\n",
        "TRUE_W = 3.5\n",
        "TRUE_b = 50.0\n",
        "NUM_EXAMPLES = 1000\n",
        "\n",
        "inputs = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "outputs = inputs * TRUE_W + TRUE_b + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJesGEUgcj4"
      },
      "source": [
        "### Loss Function\n",
        "Here we will use Mean Squared Error (MSE), because this is a regression problem. We are trying to predict a continuous target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDeUBW6k4Ri4"
      },
      "source": [
        "def loss(target_y, predicted_y):\n",
        "  \"MSE\"\n",
        "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgTf6vTS69Sw"
      },
      "source": [
        "### Neural Network Architecture\n",
        "Lets create a Neural Network class called \"Model\" to contain this functionality. Note: This is essentially a linear regression whose coefficients are trained by gradient descent. In practice, gradient descent works on much more complex function like the multi-layer networks we constructed yesterday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUI8VSR5zyBv"
      },
      "source": [
        "class Model(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = tf.Variable(8.0)\n",
        "    self.b = tf.Variable(40.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.W * x + self.b\n",
        "\n",
        "model = Model()\n",
        "\n",
        "assert model(3.0).numpy() == 64.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbyT_FJ88IlK"
      },
      "source": [
        "### Initial Weights\n",
        "The initial weights in our model were arbitrary. In practice, weights are initialized randomly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreIDe6P8H0H",
        "outputId": "5cf7a9dc-6821-40a3-8c80-0933b869040d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(inputs, outputs, c='b')\n",
        "plt.scatter(inputs, model(inputs), c='r')\n",
        "plt.show()\n",
        "\n",
        "print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Current loss: 123.462173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ujj6vNYQyX",
        "toc-hr-collapsed": true
      },
      "source": [
        "### Update Weights Based on Gradient\n",
        "\n",
        "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
        "\n",
        "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). \n",
        "\n",
        "Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
        "\n",
        "1) Stochastic Gradient Descent\n",
        "\n",
        "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgaGD6YlHoid"
      },
      "source": [
        " def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t: \n",
        "     current_loss = loss(outputs, model(inputs))\n",
        "  dW, db = t.gradient(current_loss, [model.W, model.b])\n",
        "  model.W.assign_sub(learning_rate * dW)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iziWWURgck8"
      },
      "source": [
        "### Train the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zn_HgFuHhTr",
        "outputId": "09a17514-d62b-401b-eb2c-7d3e26eda1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# Store Some history of weights\n",
        "Ws, bs = [], []\n",
        "epochs = range(30)\n",
        "for epoch in epochs:\n",
        "  Ws.append(model.W.numpy())\n",
        "  bs.append(model.b.numpy())\n",
        "  current_loss = loss(outputs, model(inputs))\n",
        "\n",
        "  train(model, inputs, outputs, learning_rate=0.1)\n",
        "  print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0: W=8.00 b=40.00 loss=123.46217\n",
            "Epoch  1: W=7.06 b=42.03 loss=78.64680\n",
            "Epoch  2: W=6.32 b=43.64 loss=50.23962\n",
            "Epoch  3: W=5.73 b=44.93 loss=32.23246\n",
            "Epoch  4: W=5.27 b=45.96 loss=20.81739\n",
            "Epoch  5: W=4.90 b=46.78 loss=13.58095\n",
            "Epoch  6: W=4.61 b=47.43 loss=8.99328\n",
            "Epoch  7: W=4.38 b=47.95 loss=6.08474\n",
            "Epoch  8: W=4.20 b=48.37 loss=4.24067\n",
            "Epoch  9: W=4.06 b=48.70 loss=3.07145\n",
            "Epoch 10: W=3.95 b=48.96 loss=2.33008\n",
            "Epoch 11: W=3.86 b=49.17 loss=1.85998\n",
            "Epoch 12: W=3.79 b=49.34 loss=1.56187\n",
            "Epoch 13: W=3.73 b=49.48 loss=1.37283\n",
            "Epoch 14: W=3.69 b=49.58 loss=1.25294\n",
            "Epoch 15: W=3.65 b=49.67 loss=1.17690\n",
            "Epoch 16: W=3.62 b=49.74 loss=1.12867\n",
            "Epoch 17: W=3.60 b=49.79 loss=1.09809\n",
            "Epoch 18: W=3.59 b=49.83 loss=1.07868\n",
            "Epoch 19: W=3.57 b=49.87 loss=1.06637\n",
            "Epoch 20: W=3.56 b=49.90 loss=1.05856\n",
            "Epoch 21: W=3.55 b=49.92 loss=1.05361\n",
            "Epoch 22: W=3.55 b=49.94 loss=1.05047\n",
            "Epoch 23: W=3.54 b=49.95 loss=1.04847\n",
            "Epoch 24: W=3.54 b=49.96 loss=1.04721\n",
            "Epoch 25: W=3.53 b=49.97 loss=1.04640\n",
            "Epoch 26: W=3.53 b=49.98 loss=1.04589\n",
            "Epoch 27: W=3.53 b=49.98 loss=1.04557\n",
            "Epoch 28: W=3.53 b=49.99 loss=1.04537\n",
            "Epoch 29: W=3.53 b=49.99 loss=1.04523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSEt07wdHvi2",
        "outputId": "1db05e8a-3c9e-4784-84ec-0a64740e6eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs, Ws, 'r', epochs, bs, 'b')\n",
        "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
        "         [TRUE_b] * len(epochs), 'b--')\n",
        "plt.legend(['W', 'b', 'True W', 'True b'])\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKUVGoRxgck_"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project, you will be asked to explain the logic of backpropagation and gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRsjONxQemmp"
      },
      "source": [
        "### In the following two sections we'll look at batch size and learning rate hyperparameters in isolation. \n",
        "However, it's important to know that recent research found some interesting relationships between batch sizes and learning rates. The best available suggestion today is to scale batch size proportionally to a learning rate:\n",
        "- https://openreview.net/pdf?id=B1Yy1BxCZ\n",
        "- https://papers.nips.cc/paper/8398-control-batch-size-and-learning-rate-to-generalize-well-theoretical-and-empirical-evidence.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTqZg-6igclA",
        "toc-hr-collapsed": true
      },
      "source": [
        "# Batch Size (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrm-racgclA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
        "\n",
        "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. \n",
        "\n",
        "### Batch Size\n",
        "Batches are the number of observations our model is shown to make predictions and update the weights. Batches are selected randomly during epoch. All observations are considered when passing thru an epoch at some point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNQ2ZCi7I4i6"
      },
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjW2lYVI9Q2",
        "outputId": "d9e0d0cd-ddd5-432e-ffcf-763af5899136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "X_train = X_train.reshape((60000, 784))\n",
        "X_test = X_test.reshape((10000, 784))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7x17kDKJSy5"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "def create_model(lr = 0.01):\n",
        "  opt = SGD(learning_rate=lr)\n",
        "  model = Sequential([\n",
        "                      Dense(32, activation='relu', input_dim=784),\n",
        "                      Dense(32, activation='relu'),\n",
        "                      Dense(10, activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W74jGlO7zPsk"
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECwcifJzPqO",
        "outputId": "1124693f-1b17-49a5-8813-a205bb5254d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 32)                25120     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 26,506\n",
            "Trainable params: 26,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7UE-KluPsX"
      },
      "source": [
        "## Follow Along\n",
        "Let's run a series of experiments for a default, small, and large batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhpDaVFRJl3U"
      },
      "source": [
        "### Default\n",
        "Batch Size is 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-ChVGikgclD",
        "outputId": "c58fcb45-820f-43d2-8080-4281706616d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bt_default = model.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test,y_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7560 - accuracy: 0.7892 - val_loss: 0.3701 - val_accuracy: 0.8965\n",
            "Epoch 2/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3378 - accuracy: 0.9044 - val_loss: 0.3034 - val_accuracy: 0.9102\n",
            "Epoch 3/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2867 - accuracy: 0.9182 - val_loss: 0.2621 - val_accuracy: 0.9240\n",
            "Epoch 4/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2540 - accuracy: 0.9271 - val_loss: 0.2492 - val_accuracy: 0.9261\n",
            "Epoch 5/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2302 - accuracy: 0.9340 - val_loss: 0.2164 - val_accuracy: 0.9380\n",
            "Epoch 6/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2097 - accuracy: 0.9401 - val_loss: 0.2016 - val_accuracy: 0.9416\n",
            "Epoch 7/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1937 - accuracy: 0.9448 - val_loss: 0.1907 - val_accuracy: 0.9445\n",
            "Epoch 8/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1797 - accuracy: 0.9485 - val_loss: 0.1766 - val_accuracy: 0.9487\n",
            "Epoch 9/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1679 - accuracy: 0.9520 - val_loss: 0.1673 - val_accuracy: 0.9504\n",
            "Epoch 10/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1570 - accuracy: 0.9549 - val_loss: 0.1592 - val_accuracy: 0.9517\n",
            "Epoch 11/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1474 - accuracy: 0.9580 - val_loss: 0.1499 - val_accuracy: 0.9549\n",
            "Epoch 12/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1399 - accuracy: 0.9597 - val_loss: 0.1491 - val_accuracy: 0.9559\n",
            "Epoch 13/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1323 - accuracy: 0.9615 - val_loss: 0.1405 - val_accuracy: 0.9586\n",
            "Epoch 14/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1263 - accuracy: 0.9635 - val_loss: 0.1374 - val_accuracy: 0.9593\n",
            "Epoch 15/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1206 - accuracy: 0.9650 - val_loss: 0.1312 - val_accuracy: 0.9612\n",
            "Epoch 16/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1160 - accuracy: 0.9664 - val_loss: 0.1300 - val_accuracy: 0.9611\n",
            "Epoch 17/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1111 - accuracy: 0.9678 - val_loss: 0.1279 - val_accuracy: 0.9623\n",
            "Epoch 18/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1071 - accuracy: 0.9688 - val_loss: 0.1243 - val_accuracy: 0.9628\n",
            "Epoch 19/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1033 - accuracy: 0.9703 - val_loss: 0.1229 - val_accuracy: 0.9645\n",
            "Epoch 20/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0997 - accuracy: 0.9711 - val_loss: 0.1203 - val_accuracy: 0.9660\n",
            "Epoch 21/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0962 - accuracy: 0.9721 - val_loss: 0.1222 - val_accuracy: 0.9655\n",
            "Epoch 22/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0931 - accuracy: 0.9731 - val_loss: 0.1205 - val_accuracy: 0.9650\n",
            "Epoch 23/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0901 - accuracy: 0.9743 - val_loss: 0.1151 - val_accuracy: 0.9663\n",
            "Epoch 24/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0876 - accuracy: 0.9749 - val_loss: 0.1155 - val_accuracy: 0.9664\n",
            "Epoch 25/25\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0850 - accuracy: 0.9747 - val_loss: 0.1163 - val_accuracy: 0.9659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvsbOFnDJuG0"
      },
      "source": [
        "### Small Batch Size\n",
        "Batch Size is 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZtO3_t3G3n"
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diDzvb-UJ1je",
        "outputId": "4be508e2-b1bf-42ab-d559-6593de97430d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bt_small = model.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=25,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_test,y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.4181 - accuracy: 0.8811 - val_loss: 0.2307 - val_accuracy: 0.9292\n",
            "Epoch 2/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.2167 - accuracy: 0.9366 - val_loss: 0.1956 - val_accuracy: 0.9396\n",
            "Epoch 3/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.1693 - accuracy: 0.9495 - val_loss: 0.1721 - val_accuracy: 0.9494\n",
            "Epoch 4/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.1433 - accuracy: 0.9569 - val_loss: 0.1385 - val_accuracy: 0.9578\n",
            "Epoch 5/25\n",
            "7500/7500 [==============================] - 17s 2ms/step - loss: 0.1252 - accuracy: 0.9625 - val_loss: 0.1302 - val_accuracy: 0.9595\n",
            "Epoch 6/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.1133 - accuracy: 0.9649 - val_loss: 0.1274 - val_accuracy: 0.9622\n",
            "Epoch 7/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.1038 - accuracy: 0.9685 - val_loss: 0.1174 - val_accuracy: 0.9650\n",
            "Epoch 8/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0963 - accuracy: 0.9705 - val_loss: 0.1273 - val_accuracy: 0.9611\n",
            "Epoch 9/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0884 - accuracy: 0.9728 - val_loss: 0.1071 - val_accuracy: 0.9672\n",
            "Epoch 10/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0826 - accuracy: 0.9744 - val_loss: 0.1035 - val_accuracy: 0.9681\n",
            "Epoch 11/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0770 - accuracy: 0.9767 - val_loss: 0.1129 - val_accuracy: 0.9661\n",
            "Epoch 12/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0737 - accuracy: 0.9776 - val_loss: 0.1076 - val_accuracy: 0.9683\n",
            "Epoch 13/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0690 - accuracy: 0.9789 - val_loss: 0.1035 - val_accuracy: 0.9675\n",
            "Epoch 14/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0652 - accuracy: 0.9796 - val_loss: 0.1088 - val_accuracy: 0.9676\n",
            "Epoch 15/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0615 - accuracy: 0.9809 - val_loss: 0.1017 - val_accuracy: 0.9686\n",
            "Epoch 16/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0575 - accuracy: 0.9819 - val_loss: 0.0996 - val_accuracy: 0.9713\n",
            "Epoch 17/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0549 - accuracy: 0.9826 - val_loss: 0.1078 - val_accuracy: 0.9692\n",
            "Epoch 18/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.1088 - val_accuracy: 0.9683\n",
            "Epoch 19/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0504 - accuracy: 0.9845 - val_loss: 0.1048 - val_accuracy: 0.9716\n",
            "Epoch 20/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0476 - accuracy: 0.9848 - val_loss: 0.1168 - val_accuracy: 0.9686\n",
            "Epoch 21/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0453 - accuracy: 0.9858 - val_loss: 0.1007 - val_accuracy: 0.9714\n",
            "Epoch 22/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0440 - accuracy: 0.9859 - val_loss: 0.1090 - val_accuracy: 0.9702\n",
            "Epoch 23/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.1150 - val_accuracy: 0.9687\n",
            "Epoch 24/25\n",
            "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0401 - accuracy: 0.9874 - val_loss: 0.1148 - val_accuracy: 0.9689\n",
            "Epoch 25/25\n",
            "7500/7500 [==============================] - 17s 2ms/step - loss: 0.0374 - accuracy: 0.9882 - val_loss: 0.1178 - val_accuracy: 0.9686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iPvvvt5J2Xl"
      },
      "source": [
        "### Large Batch Size\n",
        "Batch Size is 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h8Z5293KABT"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9AOFE95Zs2",
        "outputId": "8a7858f2-388b-4bb1-ef67-314c9daf2ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = create_model()\n",
        "bt_large = model.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=25,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_test,y_test))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "118/118 [==============================] - 1s 4ms/step - loss: 2.1978 - accuracy: 0.2248 - val_loss: 2.0228 - val_accuracy: 0.3730\n",
            "Epoch 2/25\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 1.8102 - accuracy: 0.4754 - val_loss: 1.5547 - val_accuracy: 0.5916\n",
            "Epoch 3/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.3322 - accuracy: 0.6679 - val_loss: 1.1077 - val_accuracy: 0.7357\n",
            "Epoch 4/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.9782 - accuracy: 0.7645 - val_loss: 0.8462 - val_accuracy: 0.7918\n",
            "Epoch 5/25\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 0.7824 - accuracy: 0.8021 - val_loss: 0.7030 - val_accuracy: 0.8182\n",
            "Epoch 6/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.8239 - val_loss: 0.6134 - val_accuracy: 0.8415\n",
            "Epoch 7/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.5946 - accuracy: 0.8413 - val_loss: 0.5519 - val_accuracy: 0.8525\n",
            "Epoch 8/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.8535 - val_loss: 0.5059 - val_accuracy: 0.8630\n",
            "Epoch 9/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.8635 - val_loss: 0.4718 - val_accuracy: 0.8718\n",
            "Epoch 10/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.4704 - accuracy: 0.8712 - val_loss: 0.4425 - val_accuracy: 0.8794\n",
            "Epoch 11/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.8765 - val_loss: 0.4201 - val_accuracy: 0.8856\n",
            "Epoch 12/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8815 - val_loss: 0.4029 - val_accuracy: 0.8895\n",
            "Epoch 13/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.4074 - accuracy: 0.8862 - val_loss: 0.3864 - val_accuracy: 0.8932\n",
            "Epoch 14/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8891 - val_loss: 0.3735 - val_accuracy: 0.8967\n",
            "Epoch 15/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8918 - val_loss: 0.3618 - val_accuracy: 0.8978\n",
            "Epoch 16/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8942 - val_loss: 0.3530 - val_accuracy: 0.9003\n",
            "Epoch 17/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8970 - val_loss: 0.3447 - val_accuracy: 0.9024\n",
            "Epoch 18/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8990 - val_loss: 0.3368 - val_accuracy: 0.9033\n",
            "Epoch 19/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.9010 - val_loss: 0.3301 - val_accuracy: 0.9047\n",
            "Epoch 20/25\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 0.3392 - accuracy: 0.9026 - val_loss: 0.3251 - val_accuracy: 0.9071\n",
            "Epoch 21/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3333 - accuracy: 0.9040 - val_loss: 0.3190 - val_accuracy: 0.9086\n",
            "Epoch 22/25\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 0.3279 - accuracy: 0.9057 - val_loss: 0.3139 - val_accuracy: 0.9097\n",
            "Epoch 23/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.9071 - val_loss: 0.3085 - val_accuracy: 0.9121\n",
            "Epoch 24/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3181 - accuracy: 0.9082 - val_loss: 0.3046 - val_accuracy: 0.9121\n",
            "Epoch 25/25\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.9093 - val_loss: 0.3005 - val_accuracy: 0.9140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ujUz6BKUGz"
      },
      "source": [
        "### Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5DOZNMKYt-",
        "outputId": "fca9a7a8-7c64-4171-f155-5e69a75af8d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "batch_sizes = []\n",
        "for exp, result in zip([bt_default, bt_small, bt_large], [\"32_\", \"8_\", \"512_\"]):\n",
        "    df = pd.DataFrame.from_dict(exp.history)\n",
        "    df['epoch'] = df.index.values\n",
        "    df['Batch Size'] = result\n",
        "    batch_sizes.append(df)\n",
        "df = pd.concat(batch_sizes)\n",
        "df['Batch Size'] = df['Batch Size'].astype('str')\n",
        "df"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "      <th>Batch Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.756028</td>\n",
              "      <td>0.789233</td>\n",
              "      <td>0.370060</td>\n",
              "      <td>0.8965</td>\n",
              "      <td>0</td>\n",
              "      <td>32_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.337757</td>\n",
              "      <td>0.904400</td>\n",
              "      <td>0.303405</td>\n",
              "      <td>0.9102</td>\n",
              "      <td>1</td>\n",
              "      <td>32_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.286692</td>\n",
              "      <td>0.918167</td>\n",
              "      <td>0.262141</td>\n",
              "      <td>0.9240</td>\n",
              "      <td>2</td>\n",
              "      <td>32_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.254022</td>\n",
              "      <td>0.927117</td>\n",
              "      <td>0.249249</td>\n",
              "      <td>0.9261</td>\n",
              "      <td>3</td>\n",
              "      <td>32_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.230184</td>\n",
              "      <td>0.934017</td>\n",
              "      <td>0.216431</td>\n",
              "      <td>0.9380</td>\n",
              "      <td>4</td>\n",
              "      <td>32_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.333260</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.319035</td>\n",
              "      <td>0.9086</td>\n",
              "      <td>20</td>\n",
              "      <td>512_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.327864</td>\n",
              "      <td>0.905700</td>\n",
              "      <td>0.313896</td>\n",
              "      <td>0.9097</td>\n",
              "      <td>21</td>\n",
              "      <td>512_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.322857</td>\n",
              "      <td>0.907117</td>\n",
              "      <td>0.308543</td>\n",
              "      <td>0.9121</td>\n",
              "      <td>22</td>\n",
              "      <td>512_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.318099</td>\n",
              "      <td>0.908150</td>\n",
              "      <td>0.304578</td>\n",
              "      <td>0.9121</td>\n",
              "      <td>23</td>\n",
              "      <td>512_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.313708</td>\n",
              "      <td>0.909283</td>\n",
              "      <td>0.300546</td>\n",
              "      <td>0.9140</td>\n",
              "      <td>24</td>\n",
              "      <td>512_</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy  epoch Batch Size\n",
              "0   0.756028  0.789233  0.370060        0.8965      0        32_\n",
              "1   0.337757  0.904400  0.303405        0.9102      1        32_\n",
              "2   0.286692  0.918167  0.262141        0.9240      2        32_\n",
              "3   0.254022  0.927117  0.249249        0.9261      3        32_\n",
              "4   0.230184  0.934017  0.216431        0.9380      4        32_\n",
              "..       ...       ...       ...           ...    ...        ...\n",
              "20  0.333260  0.904000  0.319035        0.9086     20       512_\n",
              "21  0.327864  0.905700  0.313896        0.9097     21       512_\n",
              "22  0.322857  0.907117  0.308543        0.9121     22       512_\n",
              "23  0.318099  0.908150  0.304578        0.9121     23       512_\n",
              "24  0.313708  0.909283  0.300546        0.9140     24       512_\n",
              "\n",
              "[75 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEIggsi26RyB",
        "outputId": "90bcf567-c077-48a2-f099-13632f2356fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "sns.lineplot(x='epoch', y='val_accuracy', hue = 'Batch Size', data=df)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f45a264ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZ2vUYYgclS"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with batch size on today's assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cP9Pm_gclS"
      },
      "source": [
        "# Learning Rate (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bna67ADZgclT",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview\n",
        "\n",
        "Learning Rate controls the size of the update to our weights that the optimization algorithm makes. VERY IMPORTANT hyperparameter.\n",
        "\n",
        "* Too high of a learning rate causes unstable results\n",
        "* Too Low of a learning rate the model will underfit\n",
        "* Goldy Locks parameters - it needs be \"just right\"\n",
        "* Scale of 0-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVYOn7bgcle",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Same experiment with Batch but different learning rates:\n",
        "* High Learning = .75\n",
        "* Default Learning = .01\n",
        "* Low Learning Rate = .0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI_H8Em1NOii"
      },
      "source": [
        "### Default Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8cb_ZUNVtL"
      },
      "source": [
        "model = create_model()\n",
        "lr_default = model.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQZ4SZdKNMRO"
      },
      "source": [
        "### High Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny72mU_dNWMR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqDmTVBNSMR"
      },
      "source": [
        "### Low Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ech1ER64NXBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZe6DyhANXdU"
      },
      "source": [
        "### Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-BdFdMNph-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2aiw_Sgcl7"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with different learning rates today.\n",
        "\n",
        "---"
      ]
    }
  ]
}